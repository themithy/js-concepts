On Solving DP Problems

Dynamic Programming (DP) is a pretty generic problem solving technique that has been applied in a wide range of algorithms. Examples include: the Dijkstra's algorithm for finding a shortest path in a graph, Held-Karp for solving the travelling salesman problem, and much more. Computing Fibonacci numbers in a loop is also a simple example of DP.
I will explain in my own words how this technique can be used in solving programming problems. Basically there are two features of every DP algorithm - or at least that is how I understand it. The first feature is an iteration over a collection, be it an array or a graph, which can be seen just as an array of nodes. In every step we compute some form of an aggregate which represents the current state of the problem being computed - that is the second feature of DP.  
The aggregate contains all information we need, so we are not forced to go back and forth.  Without it we would be forced to check all possible solutions which is most often than not an unacceptable solution (usually called brute-force).  The aggregate is always domain-specific so it requires thorough understanding and thinking to design it properly. For example in the Dijkstra's algorithm the aggregate is the shortest distance from the beginning node to each node.
So let us consider an example. A frog jumps from stone to stone that are some distance apart. The frog can jump at most K meters, and it can skip stones if desired. The distances between stones (in whole meters) are given in an array of length N. What is the minimum number of jumps to reach the end from the start ?
Let us approach this problem from DP perspective. Feature number one tells us to iterate over the array (hopefully only once). Now we only need the feature number two, that is the aggregate. Without it we would need to consider all possible paths, which is not acceptable.
To design the aggregate let us try to discover some properties which will help us simplify the problem. In this particular example we can observe that at any stone, the future path is independent from the past one. In other means it does not matter how we got to a particular stone, in only matters how many jumps we had to made. This is the independence property. That will allow us to look retrospectively towards the stones from which we could have made the jump to the current stone - and compute the minimum of the number of jumps, incremented by one.
So we can create another array of the same length N, and for each stone to keep the minimum number of jumps that can bring us to that stone. Now by combining feature one and two we can create the final algorithm. At every stone we store the minimum number of jumps from the beginning, which is the minimum of the distances to all reachable past stones (plus one of course).
This algorithm has the worst time complexity O(N*M), which means it is linear in the number of stones (N), which is a pretty decent result.
That said we have shown how to apply the DP to solving programming problems, other techniques exist too, but this is by far the most used.
